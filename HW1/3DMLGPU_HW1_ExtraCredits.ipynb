{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuock0129/GPU-Accelerated-3D-Machine-Learning/blob/main/HW1/3DMLGPU_HW1_ExtraCredits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXTRA CREDIT-1: Comparison between models and datasets (30 points)\n",
        "\n",
        "*Before attempting Extra Credits you must complete both Part-1 and Part-2 of HW1. Otherwise no bonus points will be awarded.*\n",
        "\n",
        "Now you have used 4 datasets (ShapeNetCore, ModelNet40, S3DIS, and Objaverse) and 2 models, in both classification and segmentation tasks. Create a table to clearly depict the performance of models across different datasets and different tasks.\n",
        "\n",
        "Note: We are not expecting you to train/test on all models across all datasets and all tasks, but attempt as much as you can possibly do.\n",
        "\n",
        "#### Bonus points:\n",
        "1. Compare the performance of the trained models across as many datasets as possible - partial credit will be awarded (20 pt)\n",
        "2. Create a standard test set for classification, and another for segmentation. Compare each model's performance across this standard test set. (10 pt)\n",
        "\n",
        "Also answer the following questions:\n",
        "- Do you think this comparison table is a true depiction of model capability? Are there any limitations in this comparative analysis?\n",
        "- Have you seen any pattern in the results? if so, what?\n",
        "- What did you learn from this assignment? whar were the most challenging parts? which portions needed the most debugging/troubleshooting?\n"
      ],
      "metadata": {
        "id": "N7ortgtva_62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KEY4_f94bBVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Credit-2 (8 points): 3D Deep Learning Metrics\n",
        "\n",
        "*Before attempting Extra Credits you must complete both Part-1 and Part-2 of HW1. Otherwise no bonus points will be awarded.*\n",
        "\n",
        "This question is a general knowledge check about 3D metrics (no coding required). Only attempt this if you have completed all mandatory portions. Otherwise this bonus section will not be counted. In 3D shape tasks, especially generative or segmentation tasks, we often use specialized evaluation metrics:\n",
        "\n",
        "Chamfer Distance (CD): a measurement of similarity between two point sets (e.g., a predicted point cloud vs a target point cloud).\n",
        "\n",
        "Earth Mover’s Distance (EMD): another distance between two sets of points, which finds an optimal one-to-one correspondence (it is more computationally expensive than CD).\n",
        "\n",
        "Intersection over Union (IoU): commonly used for evaluating 3D segmentation or volumetric predictions (voxel IoU), comparing predicted vs ground-truth occupancy.\n",
        "\n",
        "Mesh quality metrics: if the output is a mesh, one might evaluate aspects like mesh completeness, smoothness, or use point cloud distances between mesh surfaces.\n",
        "\n",
        "**Questions**:\n",
        "\n",
        "1. Explain in your own words what Chamfer Distance and Earth Mover’s Distance measure. In what context would you use these metrics? (e.g., evaluating a reconstructed point cloud against the original)\n",
        "\n",
        "2. Explain what mean IoU signifies in a segmentation task. How is IoU computed for 3D segmentation or object part segmentation?\n",
        "\n",
        "\n",
        "3. If you were evaluating a method that outputs 3D meshes, name one or two criteria you might use to assess the mesh quality (aside from CD/EMD, which can also be applied after sampling points from the mesh).\n"
      ],
      "metadata": {
        "id": "rBi8SnVWbowN"
      }
    }
  ]
}