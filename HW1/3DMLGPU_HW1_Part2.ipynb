{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuock0129/GPU-Accelerated-3D-Machine-Learning/blob/main/HW1/3DMLGPU_HW1_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d053e8",
      "metadata": {
        "id": "d3d053e8"
      },
      "source": [
        "# 3DMLGPU - HW1 (Part 2)\n",
        "In the 2nd part of HW1 assignment, we will continue our study of 3D ML datasets. We will work with objects in the mesh representation directly, as well as converting the object into other representations, such as a sampled point cloud and voxel grid.\n",
        "\n",
        "Regarding QnA: Put your answers/results/analysis in a separate word/pdf report and submit along with this completed notebook. We are expecting brief, to-the-point answers for the questions asked in this notebook.\n",
        "\n",
        "Note: You may not be able to run the model in one sitting. It is recommended that you connect your google drive to periodically save the model weights/visualizations/results etc. Complete one section at a time.\n",
        "\n",
        "\n",
        "## Objaverse dataset\n",
        "Objaverse is a very large collection of 3D objects (over 800,000 models in Objaverse 1.0). In this assignment, rather than using the entire dataset, we will select a manageable subset of object categories to work with, due to resource and time constraints. We will use the Objaverse Python API (via HuggingFace) to fetch object metadata and data. First, let's install and import the necessary libraries for accessing Objaverse and processing 3D models (point cloud sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d32988e",
      "metadata": {
        "id": "3d32988e"
      },
      "outputs": [],
      "source": [
        "!pip install -q objaverse trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import trimesh\n",
        "import torch"
      ],
      "metadata": {
        "id": "8KMZMor8br0M"
      },
      "id": "8KMZMor8br0M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we use the Objaverse API to load the dataset annotations. Objaverse provides a mapping of LVIS categories (a set of ~1200 object classes from an image dataset) to lists of object UIDs (unique identifiers for each 3D model). We will use this to understand the class coverage of Objaverse and to pick a subset of classes for our training."
      ],
      "metadata": {
        "id": "Mfr889HKOoWO"
      },
      "id": "Mfr889HKOoWO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204d4db2",
      "metadata": {
        "id": "204d4db2"
      },
      "outputs": [],
      "source": [
        "import objaverse\n",
        "\n",
        "# Load all object UIDs (just to know how many objects are in Objaverse)\n",
        "uids = objaverse.load_uids()\n",
        "print(f\"Total number of objects in Objaverse: {len(uids)}\")\n",
        "\n",
        "# Load the LVIS category annotations (may take a few minutes)\n",
        "lvis_annotations = objaverse.load_lvis_annotations()\n",
        "print(f\"Total number of LVIS categories in Objaverse: {len(lvis_annotations)}\")\n",
        "\n",
        "# Example: list a few categories and the number of objects in each\n",
        "categories = list(lvis_annotations.keys())\n",
        "print(\"Sample categories and object counts:\")\n",
        "for cat in categories[:5]:\n",
        "    print(f\"  {cat}: {len(lvis_annotations[cat])} objects\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a280f4f",
      "metadata": {
        "id": "6a280f4f"
      },
      "source": [
        "## Task-4, Part-1 (10 points)\n",
        "Select any 2 categories from `lvis_annotations`, visualize a sample point cloud and a voxel grid from each, and answer the following questions:\n",
        "- Which library did you use to visualize the point cloud?\n",
        "- How many object categories are there in Objaverse (LVIS annotations)? Are the categories balanced?\n",
        "- What file format are the models stored in?\n",
        "- Any other insights?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4cd710",
      "metadata": {
        "id": "ce4cd710"
      },
      "outputs": [],
      "source": [
        "# [TODO] Your code here: select 1-2 categories, download one sample object from each,\n",
        "# convert to point cloud and visualize it.\n",
        "\n",
        "# HINT:\n",
        "# - Choose a category, e.g., category_name = \"Chair\" (ensure it exists in lvis_annotations keys).\n",
        "# - Pick one UID from lvis_annotations[category_name].\n",
        "# - Download the object: paths = objaverse.load_objects([sample_uid]); path = paths[sample_uid]\n",
        "# - Load the mesh (e.g., using trimesh): import trimesh; mesh = trimesh.load(path)\n",
        "#   (If the mesh is a scene with multiple parts, you may need to merge them into one mesh.\n",
        "#    e.g., mesh = trimesh.util.concatenate(list(mesh.geometry.values())) if mesh is a scene.)\n",
        "# - Sample points: pts = mesh.sample(NUM_POINTS)  # e.g., NUM_POINTS = 1024\n",
        "# - Visualize pts in 3D: you can use plotly.graph_objects.Scatter3d, or matplotlib 3D, etc.\n",
        "\n",
        "# [TODO] Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece19275",
      "metadata": {
        "id": "ece19275"
      },
      "source": [
        "[TODO]: Your analysis here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7a77dfc",
      "metadata": {
        "id": "e7a77dfc"
      },
      "source": [
        "## Task-4, Part 2 (13 points): Data Preparation\n",
        "\n",
        "Now we will prepare a dataset of point clouds and labels from Objaverse for training our models. Choose a subset of object categories to use for the classification task (select 10 categories that have a large number of objects to ensure sufficient training data per class. Explain your reasoning when making this selection.). List your chosen categories. Using the lvis_annotations, gather all object UIDs for those categories. Next, split the objects in each chosen category into a training set and a test/validation set. A common split is 80% for training and 20% for testing, but you can choose an appropriate split (ensure that objects of the same category are divided and none of the test objects appear in training). For each object in the training and test sets:\n",
        "\n",
        "1. Download the object file using objaverse.load_objects.\n",
        "\n",
        "2. Load the mesh and sample a point cloud of a fixed size (e.g., 1024 or 2048 points) from the surface of the mesh. You should also apply preprocessing such as normalizing the point cloud (e.g., centering at the origin and scaling to unit sphere) so that all objects have a similar scale and position.\n",
        "\n",
        "3. Convert the sampled point cloud into a dense voxel grid representation.\n",
        "\n",
        "4. For a few object instances, generate 2D visualizations of the object in all three representations: mesh, point cloud, and voxel.  Include the figures in a side-by-side plot presentation, with appropriate caption/labels.\n",
        "\n",
        "(Optional but recommended): You may also compute point normals if needed, but for this assignment we will primarily use just the coordinates (XYZ) as input features. PointNet/PointNet++ can use normals as additional features, but handling normals is not required here.\n",
        "\n",
        "(Optional but recommended) Data augmentation: Consider applying random transformations to the point clouds for training robustness â€“ for example, random rotations about the upright axis, slight jitter/noise to points, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd198ceb",
      "metadata": {
        "id": "bd198ceb"
      },
      "outputs": [],
      "source": [
        "# [TODO] Your code here: Prepare the data\n",
        "# Steps:\n",
        "# 1. Select subset of categories\n",
        "selected_categories = [ /* e.g., \"Chair\", \"Table\", \"Lamp\", ... */ ]\n",
        "\n",
        "# 2. Collect UIDs for each selected category\n",
        "selected_uids = {cat: lvis_annotations[cat] for cat in selected_categories}\n",
        "\n",
        "# 3. Split into train/test lists for each category\n",
        "train_uids = []\n",
        "test_uids = []\n",
        "train_labels = []\n",
        "test_labels = []\n",
        "\n",
        "# [TODO] your code here\n",
        "# delete the below commented portion\n",
        "# for label, cat in enumerate(selected_categories):\n",
        "#     uids_list = selected_uids[cat]\n",
        "#     # Shuffle and split\n",
        "#     import random\n",
        "#     random.shuffle(uids_list)\n",
        "#     split_idx = int(0.8 * len(uids_list))\n",
        "#     train_ids_cat = uids_list[:split_idx]\n",
        "#     test_ids_cat = uids_list[split_idx:]\n",
        "#     # Add to global lists with label\n",
        "#     train_uids.extend(train_ids_cat)\n",
        "#     test_uids.extend(test_ids_cat)\n",
        "#     train_labels.extend([label] * len(train_ids_cat))\n",
        "#     test_labels.extend([label] * len(test_ids_cat))\n",
        "\n",
        "print(f\"Total training samples: {len(train_uids)}; Total test samples: {len(test_uids)}\")\n",
        "\n",
        "# 4. Download and process training objects\n",
        "NUM_POINTS = 1024  # number of points to sample per object\n",
        "train_points = []\n",
        "# [TODO] your code here\n",
        "# for uid in train_uids:\n",
        "    # path = objaverse.load_objects([uid])[uid]\n",
        "    # mesh = trimesh.load(path)\n",
        "    # # If the object is a scene with multiple geometries, merge them\n",
        "    # if isinstance(mesh, trimesh.Scene):\n",
        "    #     # Merge all geometry into a single mesh\n",
        "    #     mesh = trimesh.util.concatenate([trimesh.Trimesh(vertices=g.vertices, faces=g.faces)\n",
        "    #                                      for g in mesh.geometry.values()])\n",
        "    # # Sample points uniformly from the surface\n",
        "    # points = mesh.sample(NUM_POINTS)\n",
        "    # # Normalize the point cloud (center and scale)\n",
        "    # points = points - points.mean(axis=0)\n",
        "    # points = points / np.max(np.linalg.norm(points, axis=1))\n",
        "    # train_points.append(points.astype(np.float32))\n",
        "\n",
        "# 5. Download and process test objects (similar to training, but no need for augmentation)\n",
        "test_points = []\n",
        "# [TODO] your code here\n",
        "# for uid in test_uids:\n",
        "#     path = objaverse.load_objects([uid])[uid]\n",
        "#     mesh = trimesh.load(path)\n",
        "#     if isinstance(mesh, trimesh.Scene):\n",
        "#         mesh = trimesh.util.concatenate([trimesh.Trimesh(vertices=g.vertices, faces=g.faces)\n",
        "#                                          for g in mesh.geometry.values()])\n",
        "#     points = mesh.sample(NUM_POINTS)\n",
        "#     points = points - points.mean(axis=0)\n",
        "#     points = points / np.max(np.linalg.norm(points, axis=1))\n",
        "#     test_points.append(points.astype(np.float32))\n",
        "\n",
        "# Convert to PyTorch tensors and create DataLoaders\n",
        "train_points = torch.tensor(np.array(train_points))  # shape: (N_train, NUM_POINTS, 3)\n",
        "train_labels = torch.tensor(np.array(train_labels))\n",
        "test_points = torch.tensor(np.array(test_points))\n",
        "test_labels = torch.tensor(np.array(test_labels))\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_points, train_labels)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_points, test_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Data preparation done.\")\n",
        "print(f\"Train loader batches: {len(train_loader)}; Test loader batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions (analysis): Now answer the following based on your data preparation:\n",
        "\n",
        "1. Which categories did you select for training, and how many samples does each have? (Provide the count per category in your training set.)\n",
        "\n",
        "2. Describe the preprocessing steps you applied to the raw 3D data. Why is it important to normalize the point clouds (centering and scaling) before training?\n",
        "\n",
        "3. Did you apply any data augmentations to the training point clouds? If yes, what types and why? If not, what augmentations could be beneficial for point cloud data?\n",
        "\n",
        "4. Discuss any challenges you encountered in converting Objaverse models to point clouds (for example, issues with certain file types, very complex models, etc., and how you handled them)."
      ],
      "metadata": {
        "id": "ZAkux4umQuVV"
      },
      "id": "ZAkux4umQuVV"
    },
    {
      "cell_type": "markdown",
      "id": "e0fd0248",
      "metadata": {
        "id": "e0fd0248"
      },
      "source": [
        "## Task-5, Part 1 (15 points): Fine-tuning PointNet\n",
        "We will now load a pre-trained PointNet model and fine-tune it on our Objaverse subset. In this part, we focus on object classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d33aeba",
      "metadata": {
        "id": "0d33aeba"
      },
      "outputs": [],
      "source": [
        "# Clone the PointNet/PointNet++ repository (if not already done)\n",
        "!git clone https://github.com/yanx27/Pointnet_Pointnet2_pytorch.git\n",
        "%cd Pointnet_Pointnet2_pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set up the PointNet model for classification. if you can load weights from a previous training (e.g., a model trained on ModelNet40 or ShapeNet), load them. If not, you can initialize from scratch."
      ],
      "metadata": {
        "id": "evrd9xRCRc6i"
      },
      "id": "evrd9xRCRc6i"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models import pointnet_cls\n",
        "\n",
        "# Initialize PointNet model for classification with the number of classes in our subset\n",
        "num_classes = len(selected_categories)\n",
        "pointnet_model = pointnet_cls.get_model(num_classes, normal_channel=False)  # normal_channel=False since we are not using normals\n",
        "pointnet_model = pointnet_model.cuda()  # move to GPU if available\n",
        "\n",
        "# Load pre-trained weights (if available)\n",
        "pretrained_path = 'path/to/pretrained/pointnet_model.pth'  # TODO: update with actual path if you have it\n",
        "try:\n",
        "    pointnet_model.load_state_dict(torch.load(pretrained_path))\n",
        "    print(\"Loaded pre-trained PointNet weights.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Pre-trained weights not found, proceeding with random initialization.\")\n"
      ],
      "metadata": {
        "id": "nF266v53Rhzu"
      },
      "id": "nF266v53Rhzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, let's prepare our training loop. We will use an optimizer (e.g., Adam) and a loss function (cross-entropy for classification). We will also log training progress to TensorBoard for visualization of loss/accuracy curves."
      ],
      "metadata": {
        "id": "HA-ilwCIRlb2"
      },
      "id": "HA-ilwCIRlb2"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Set up optimizer and loss\n",
        "optimizer = optim.Adam(pointnet_model.parameters(), lr=0.001)\n",
        "num_epochs = 5  # you can increase this for better results if time permits\n",
        "\n",
        "# TensorBoard writer for logging\n",
        "writer = SummaryWriter(log_dir=\"logs/pointnet_finetune\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    pointnet_model.train()\n",
        "    # [TODO] your training code here\n",
        "\n",
        "    # Evaluate on test set\n",
        "    pointnet_model.eval()\n",
        "    # [TODO] your test set eval code here\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    writer.add_scalar('PointNet/Loss', train_loss, epoch)\n",
        "    writer.add_scalar('PointNet/Train_Acc', train_acc, epoch)\n",
        "    writer.add_scalar('PointNet/Test_Acc', test_acc, epoch)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "h1zt3jJMRpC_"
      },
      "id": "h1zt3jJMRpC_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, let's evaluate the fine-tuned model on the test set and compute the overall accuracy:"
      ],
      "metadata": {
        "id": "Ukof1XxJRxRr"
      },
      "id": "Ukof1XxJRxRr"
    },
    {
      "cell_type": "code",
      "source": [
        "pointnet_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        # points, labels = data\n",
        "        # points = points.transpose(2,1).cuda()\n",
        "        # labels = labels.cuda()\n",
        "        # preds, _ = pointnet_model(points)\n",
        "        # _, predicted = torch.max(preds.data, 1)\n",
        "        # total += labels.size(0)\n",
        "        # correct += (predicted == labels).sum().item()\n",
        "        # all_preds.extend(predicted.cpu().numpy().tolist())\n",
        "        # all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "pointnet_test_accuracy = correct / total\n",
        "print(f\"PointNet test accuracy on Objaverse subset: {pointnet_test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Qz9YraBISA0Q"
      },
      "id": "Qz9YraBISA0Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(If you want to monitor training progress, you can run the following in a separate cell to launch TensorBoard: %load_ext tensorboard then %tensorboard --logdir logs)\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. What test accuracy did you achieve after fine-tuning PointNet on your Objaverse subset?\n",
        "\n",
        "2. Is the model overfitting or underfitting? (Hint: compare training vs test accuracy, and also consider the trend of the loss curves)\n",
        "\n",
        "3. If you started with pre-trained weights, how did the fine-tuning progress compare to what you would expect from training from scratch? (If you didnt have pre-trained weights and trained from scratch, comment on how quickly the model learned and if more epochs would likely improve results.)"
      ],
      "metadata": {
        "id": "2kQnJGgBSBtH"
      },
      "id": "2kQnJGgBSBtH"
    },
    {
      "cell_type": "markdown",
      "id": "aaeb8a96",
      "metadata": {
        "id": "aaeb8a96"
      },
      "source": [
        "## Task-5, Part 2 (15 points): Fine-tuning PointNet++\n",
        "\n",
        "Now we will fine-tune PointNet++, on the same data and compare its performance to PointNet. Set up the PointNet++ model for classification. The repository we cloned has implementations for PointNet++ (single-scale grouping (SSG) and multi-scale grouping (MSG); we can use SSG for simplicity). We will load a pre-trained PointNet++ model if available (for example, one trained on ModelNet40), similar to PointNet above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc191c15",
      "metadata": {
        "id": "bc191c15"
      },
      "outputs": [],
      "source": [
        "from models import pointnet2_cls_ssg\n",
        "\n",
        "# Initialize PointNet++ (SSG) model\n",
        "pointnet2_model = pointnet2_cls_ssg.get_model(num_classes, normal_channel=False)\n",
        "pointnet2_model = pointnet2_model.cuda()\n",
        "\n",
        "# Load pre-trained weights if available\n",
        "pretrained_path = 'path/to/pretrained/pointnet2_model.pth'  # TODO: update if available\n",
        "try:\n",
        "    pointnet2_model.load_state_dict(torch.load(pretrained_path))\n",
        "    print(\"Loaded pre-trained PointNet++ weights.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Pre-trained weights not found for PointNet++, using random init.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the PointNet++ model on the Objaverse subset, using a similar training loop as before. (Note: PointNet++ has more parameters and may train slower. You might consider using fewer epochs or a lower learning rate for fine-tuning.)"
      ],
      "metadata": {
        "id": "bHg87ucNSdq9"
      },
      "id": "bHg87ucNSdq9"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(pointnet2_model.parameters(), lr=0.001)\n",
        "writer2 = SummaryWriter(log_dir=\"logs/pointnet2_finetune\")\n",
        "num_epochs = 5  # adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # [TODO] your code here\n",
        "    # train loop\n",
        "\n",
        "    # # Evaluate on test set\n",
        "\n",
        "    # writer2.add_scalar('PointNet++/Loss', train_loss, epoch)\n",
        "    # writer2.add_scalar('PointNet++/Train_Acc', train_acc, epoch)\n",
        "    # writer2.add_scalar('PointNet++/Test_Acc', test_acc, epoch)\n",
        "    # print(f\"[PointNet++] Epoch {epoch+1}/{num_epochs}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "jm9CnXRsSeoK"
      },
      "id": "jm9CnXRsSeoK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fine-tuning, evaluate PointNet++ on the test set:"
      ],
      "metadata": {
        "id": "IrGFE-jfSo2U"
      },
      "id": "IrGFE-jfSo2U"
    },
    {
      "cell_type": "code",
      "source": [
        "pointnet2_model.eval()\n",
        "correct = 0; total = 0\n",
        "# [TODO] your code here\n",
        "# all_preds2 = []; all_labels2 = []\n",
        "# with torch.no_grad():\n",
        "#     for data in test_loader:\n",
        "#         points, labels = data\n",
        "#         points = points.transpose(2,1).cuda()\n",
        "#         labels = labels.cuda()\n",
        "#         preds = pointnet2_model(points)\n",
        "#         _, predicted = torch.max(preds.data, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         all_preds2.extend(predicted.cpu().numpy().tolist())\n",
        "#         all_labels2.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "pointnet2_test_accuracy = correct / total\n",
        "print(f\"PointNet++ test accuracy on Objaverse subset: {pointnet2_test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "DlU9WUGASqQB"
      },
      "id": "DlU9WUGASqQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "\n",
        "1. What test accuracy did PointNet++ achieve after fine-tuning?\n",
        "Compare PointNet++ and PointNet results: which model performed better on the Objaverse subset, and by how much (in terms of accuracy or other metrics)?\n",
        "\n",
        "2. Why do you think PointNet++ might perform differently than PointNet on this task? Consider the architectural differences (hierarchical local feature learning in PointNet++ vs. global feature in PointNet) and the complexity of the shapes.\n",
        "\n",
        "3. Did PointNet++ show any signs of overfitting or underfitting? Compare its training vs test performance, and possibly how quickly it converged relative to PointNet."
      ],
      "metadata": {
        "id": "MzkGOViASs43"
      },
      "id": "MzkGOViASs43"
    },
    {
      "cell_type": "markdown",
      "id": "7a6a3e7f",
      "metadata": {
        "id": "7a6a3e7f"
      },
      "source": [
        "## Task-6 (15 points): Visualize results and evaluate metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6a3587",
      "metadata": {
        "id": "3b6a3587"
      },
      "source": [
        "\n",
        "\n",
        "Now that we have two models (PointNet and PointNet++) fine-tuned on our Objaverse dataset, let's visualize and quantify their performance further.\n",
        "\n",
        "1. Confusion Matrix: Compute the confusion matrix for the best model on the test set. Plot or display the confusion matrix, with proper labels for the categories on axes.\n",
        "\n",
        "2. Precision, Recall, F1-score: Using the test predictions, compute the precision, recall, and F1-score for each category, as well as the overall or average values. You can use sklearn.metrics.classification_report or calculate manually from the confusion matrix.\n",
        "\n",
        "3. Training Curves: If you logged data to TensorBoard, you should have seen the training and validation accuracy curves. Summarize what the curves showed: did the models' performance plateau early or keep improving? Did one model converge faster than the other?\n",
        "\n",
        "4. Sample Predictions: (Optional) Pick a few test examples and visualize their point clouds, noting the model's predicted label vs. the true label. Are there any interesting cases of misclassification? (For example, an object that was predicted as a different class - why might the model have confused them?)\n",
        "\n",
        "\n",
        "Complete the code to compute the confusion matrix and classification metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Choose the model to evaluate (PointNet++ vs PointNet) based on which you want to analyze\n",
        "y_true = all_labels2  # true labels from test set (for PointNet++ in this example)\n",
        "y_pred = all_preds2   # predicted labels from test set (PointNet++)\n",
        "labels = list(range(num_classes))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "# Classification report (precision, recall, F1 per class)\n",
        "target_names = [cat for cat in selected_categories]\n",
        "report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "VjNivpP9TSd8"
      },
      "id": "VjNivpP9TSd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(You may visualize the confusion matrix as a heatmap for clarity if you wish, using matplotlib or seaborn. Ensure the axes are labeled with the category names.) Questions:\n",
        "\n",
        "1. Include the confusion matrix or describe it: which categories are most often confused with each other, based on the confusion matrix? Does this make sense (are those categories visually or geometrically similar)?\n",
        "\n",
        "2. Report the precision, recall, and F1-score for each class. Did the model perform evenly across all classes, or are some classes much better/worse? Provide possible explanations (e.g., class imbalance, shape complexity, etc.).\n",
        "\n",
        "3. Discuss the training curves from TensorBoard: did the training and test accuracy diverge (sign of overfitting) or track closely? Did you notice one model converging faster?\n",
        "\n",
        "4. Provide 1â€“2 examples of model predictions (if you examined some): were there any misclassifications that stood out, and why do you think the model made those errors?"
      ],
      "metadata": {
        "id": "S47SNozeTV3K"
      },
      "id": "S47SNozeTV3K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-7 (15 points): Training from scratch vs. fine-tuning (analysis)\n",
        "\n",
        " In this final part, reflect on the differences between training a model from scratch and fine-tuning a pre-trained model for 3D classification.\n",
        "\n",
        "1. If you fine-tuned from a pre-trained model, how did it benefit your training? Consider factors like initial accuracy on first epoch, speed of convergence, and final accuracy achieved, compared to what you would expect if starting from random initialization.\n",
        "\n",
        "2. If you have time, you can perform a small experiment: take the PointNet++ model and train it from scratch (random initialization) on the Objaverse subset for the same number of epochs, and compare the learning curve and final accuracy to the fine-tuned version. (This is optional; if you donâ€™t run a full experiment, answer conceptually.)\n",
        "\n",
        "3. Why is fine-tuning particularly useful when working with a very large dataset like Objaverse or when the number of training samples per class is limited?\n",
        "\n",
        "4. Compare the two models (PointNet vs PointNet++): which one would you choose if you had a limited computational budget, and which one if you needed the highest accuracy? Explain your reasoning in terms of model complexity (number of parameters, etc.) and performance.\n"
      ],
      "metadata": {
        "id": "DcHXLpZnUQLh"
      },
      "id": "DcHXLpZnUQLh"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SV452Fypb-1w"
      },
      "id": "SV452Fypb-1w",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}